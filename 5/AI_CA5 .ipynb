{"cells":[{"attachments":{},"cell_type":"markdown","metadata":{},"source":["<html>\n","<div>\n","  <img src=\"https://www.engineersgarage.com/wp-content/uploads/2021/11/TCH36-01-scaled.jpg\" width=360px width=auto style=\"vertical-align: middle;\">\n","  <span style=\"font-family: Georgia; font-size:30px; color: white;\"> <br/> University of Tehran <br/> AI_CA5 <br/> Spring 02 </span>\n","</div>\n","<span style=\"font-family: Georgia; font-size:15pt; color: white; vertical-align: middle;\"> low_mist - std id: 810100186 </span>\n","</html>\n","\n","In this notebook we are to learn about machine learning and try to anticipate price of houses."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Problem Description\n","in this problem we will learn about basics of machine learning, in order to assign prices to houses. At first we try to do that by using linear regression without any library, and then we use Scikit-Learn to do that."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Dataset\n","The `house_data.csv` file contains data about houses and their prices in one of the cities of Washington, D.C. in years 2014 and 2015. "]},{"cell_type":"code","execution_count":2,"metadata":{"id":"F3m_CaH3gXes"},"outputs":[],"source":["import pandas as pd\n","import numpy as np\n","import matplotlib.pyplot as plt\n","import seaborn as sns\n","import math\n","from copy import deepcopy\n","from sklearn.impute import KNNImputer\n","from sklearn.preprocessing import StandardScaler, MinMaxScaler\n","from sklearn.model_selection import train_test_split\n","from sklearn.linear_model import LogisticRegression\n","from sklearn.neighbors import KNeighborsClassifier\n","from sklearn.tree import DecisionTreeClassifier\n","from sklearn.model_selection import GridSearchCV\n","from sklearn.metrics import confusion_matrix\n","from sklearn.ensemble import RandomForestClassifier\n","from sklearn.metrics import accuracy_score\n","from sklearn.metrics import ConfusionMatrixDisplay\n","import category_encoders as cat_enc\n","from mlxtend.evaluate import bias_variance_decomp\n","\n","DATASET_PATH = \"assets/house_data.csv\""]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = pd.read_csv(DATASET_PATH)\n","pd.set_option(\"display.max_columns\", None)\n","df.head(10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Part1. Analysis of datasets\n","### Q1-1. Describe dataset using info and describe methods."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.info()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The `info` method returns general info about the dataframe, its data, and the data types.\n","\n","The panda's dataframe's is printed and we can see that there are 21613 entries in the dataframe.  \n","There are 26 columns and for each of them, the column's name, its data types, and the non-null count is shown.  \n","Non-null count shows how many rows have a value in a specific column.  \n","At the end, the count of each data type among the columns and the structure's memory usage is shown."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["The `describe` method shows some statistical information about the dataframe.\n","\n","Each table row reports a property of the corresponding column's data:  \n","\n","- count: The number of the data.\n","- mean: The average of the data.\n","- std: The standard deviation of the data.\n","- min: The minimum data.\n","- 25%: The first quartile of the column's data.\n","- 50%: The median of the column's data.\n","- 75%: The third quartile of the column's data.\n","- max: The maximum data."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q1-2. For each feature show number and proportion of missing values."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def missing_values(df: pd.DataFrame) -> pd.DataFrame:\n","    nan_values_count = df.isna().sum()\n","    nan_values_percent = nan_values_count / len(df)\n","    nan_values = pd.concat([nan_values_count, nan_values_percent], axis=1, keys=[\"Missing\", \"Percentage\"])\n","    return nan_values\n","    return nan_values[nan_values[\"Missing\"] != 0] # this one is better but I need to \n","                                                  # show the missing values for every value in report\n","\n","missing_values(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q1-3. Plotting the correlation graph between the features. Which features are most correlated with target?"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["plt.figure(figsize=(20, 20))\n","sns.heatmap(df.corr(numeric_only=True), annot=True, fmt=\".3f\", cmap=\"Blues\", linewidths=1, square=True)\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["To see what features have the most correlation with the outcome, we can simply use the price row in `df.corr()`."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["price_corr = df.corr(numeric_only=True)[\"price\"].drop(\"price\")\n","price_corr = price_corr[abs(price_corr) > 0.31].sort_values(ascending=False)\n","display(price_corr)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we can see, square foot-related features namely `sqft_living`, `sqft_above` and `sqft_living15`, has the most correlation with the outcome."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q1-4. Plot unique values for each feature of last part."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["NUM_OF_INTERVALS = 20\n","df_backup = deepcopy(df)\n","\n","sns.set(rc={'figure.figsize':(11.7,8.27)})\n","sns.color_palette(\"rocket\", as_cmap=True)\n","\n","for col in price_corr.index.to_list():\n","    if df[col].quantile(0.9) - df[col].quantile(0.1) < 20:\n","        sns.countplot(x=col, data=df)\n","        plt.show()\n","    else:\n","        intervals = pd.interval_range(start=df[col].quantile(0.1), end=df[col].quantile(0.9), periods=NUM_OF_INTERVALS + 1)\n","        interval_tuples = [(interval.left, interval.right) for interval in intervals]\n","        bins = pd.IntervalIndex.from_tuples(interval_tuples)\n","        df[col] = pd.cut(df[col], bins)\n","        ax = sns.countplot(x=col, data=df)\n","        ax.set_xticklabels([f'{int(np.mean(interval))}' for interval in interval_tuples])\n","        plt.show()\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q1-5. Plotting the relationship between the features using hexbin and scatter plots."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = deepcopy(df_backup)\n","\n","def plot_corr_scatter_hexbin(col):\n","    fig, axs = plt.subplots(ncols=2, figsize=(12, 4))\n","    plt.suptitle(col)\n","    axs[0].scatter(df[col], df[\"price\"])\n","    axs[1].hexbin(df[col], df[\"price\"], gridsize=20, cmap=\"Blues\")\n","    plt.subplots_adjust(wspace=0.3)\n","    plt.show()\n","\n","for col in price_corr.index:\n","    plot_corr_scatter_hexbin(col)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q1-6. Use other methods to analyze the data."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["def plot_corr_joint(col):\n","    sns.jointplot(x=col, y=\"price\", data=df, kind=\"hex\")\n","\n","for col in price_corr.index:\n","    plot_corr_joint(col)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Part 2. Preprocessing\n","\n","First we need to delete some invalid values such as negative values for number of bathrooms and so forth."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["negative_columns = [\"bedrooms\", \"bathrooms\", \"sqft_living\", \"grade\"]\n","df[negative_columns] = np.where(df[negative_columns] < 0, np.nan, df[negative_columns])"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q2-1. How to handle missing values.\n","Missing values in machine learning projects can be a significant hindrance to accurate results. However, there are multiple ways to deal with it during the preprocessing stage to minimize their impact. \n","\n","---\n","\n","**Imputation:** is the process of replacing missing values with a substitution.\n","  \n","One such technique is mean imputation, where missing values are replaced by the mean value of the non-missing data. Similarly, the median and mode of the non-missing data could be used as a substitute. Alternatively, missing values could be replaced with the value of the previous or the next observation - called forward-fill and backward-fill, respectively. Other ways can be to predict or simply put a random value into the data.\n","\n","  - *Filling with Mean:*  \n","    Using the average to fill missing values is simple to do and the mean is a good representative of the data as a whole.  \n","    But sometimes it may not make sense and be impossible to use for a column.\n","  - *Filling with Median:*  \n","    The outlier data can affect the mean negatively.  \n","    In such cases, it may be better to use the median which is not affected by outliers.\n","  - *Filling with Mode:*  \n","    Mean and median do not work with categorical data.  \n","    Using the mode can be a alternative for such data.\n","  - *Random Fill:*  \n","    In this method we fill using random values, mostly in the range of the column's minimum and maximum data, or between the categories.\n","  - *Predicting:*  \n","    A more advanced method is to have a way of predicting what the missing value should be based on the other properties of the row.\n","  - *Forward-fill and Backward-fill:* \n","    In this method we fill the missing values with next or previous observation respectively\n","\n","A more advanced approach is an imputation method using model-based techniques, such as k-NN imputation or MICE (Multiple Imputation by Chained Equations). While the former fills the missing data based on the k nearest neighbors, the latter fits the observed data into a regression model before imputing missing values with the help of this model.\n","\n","---\n","\n","**Dropping**: Another option is to remove any observations that contain missing values. This choice should be made with careful consideration, as it can reduce the size of the data and impact its representativeness. There are two main ways, dropping columns and dropping rows.  \n","\n","  - *Dropping Columns:*  \n","    In this method we remove any column that has missing values in it.  \n","    This is usually not wanted because we potentially losing a lot of data.  \n","    This method should only be considered on columns that have too many missing values; and in fact, its actually the better thing to do in such cases because there is not much data to fill it with good precision.\n","  - *Dropping Rows:*  \n","    Works similarly to dropping columns.  \n","    If we remove all rows that have missing values, if a column is all missing, then all of the rows will be gone.  \n","    This method should also only be considered on rows that have most of their properties missing.\n","\n","---\n","\n","Finally, in some cases where missing data is limited, one could choose to ignore these values altogether and proceed with the analysis. However, before doing that, it is essential to determine whether the missingness is random or non-random. \n","\n","These are just a few possible ways to handle missing data during the preprocessing stage of a machine learning project. Depending on the specific case, there may be other methods that could be more effective."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q2-2. Handling missing values"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["missing_values(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As we can see `yr_built` and `sqft_living` and `floors` has the most missing values.  \n","Filling with median is chosen, this is to not get affected by outliers and also not be fractional."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.fillna(df.median(numeric_only=True), inplace=True)\n","missing_values(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As an alternative method we delete rows that have more than two NaN values, and then use KNNImputer to fill them."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["missing = df_backup[df_backup.isna().sum(axis=1) > 2]\n","df_imputed = deepcopy(df_backup)\n","df_imputed.drop(missing.index, inplace=True)\n","df_imputed = df_imputed.drop([\"date\", \"location\", \"style\"], axis=1)\n","df_imputed.reset_index(drop=True, inplace=True)\n","imputer = KNNImputer(n_neighbors=5)\n","imputed = imputer.fit_transform(df_imputed)\n","imputed = pd.DataFrame(imputed, columns=df_imputed.columns)\n","imputed[negative_columns] = np.where(imputed[negative_columns] < 0, np.nan, imputed[negative_columns])"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["imputed.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["As it is shown above, both the `mean` and `std` of the features remained almost the same as the old data after filling the missing values. So, we can use the `KNNImputer` method to fill the missing values.   \n","\n","By using the `mean` method, the `mean` of the new dataset will be the same as the old one, but the `std` will change a lot. But `median` will have better performance. But we will continue with the module that we got from filling with median since we need non-numerical variables too."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q2-3. Normalization and Standardization, should we use them?"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Normalization means scaling the values of the features to a fixed range. For example, we can scale the values of the features to the range of [0, 1] or [-1, 1]. This method is useful when we have no outliers and the data lies in a fixed range. We can use the `MinMaxScaler` method to do this. We can't use normalization when we are not using algorithms such as `KNN` or `Neural Networks` which are based on distance. Below is the formula for the `MinMaxScaler` method:\n","\n","$$X_{norm} = \\frac{X - X_{min}}{X_{max} - X_{min}}$$\n","Standardization means scaling the values of the features to have a mean of 0 and a standard deviation of 1. This method is useful when we have features with different means and standard deviations. We can use the `StandardScaler` method to do this. Below is the formula for the `StandardScaler` method:\n","\n","$$X_{std} = \\frac{X - \\mu}{\\sigma}$$  \n","\n","To answer when we need to do them I quote from [this link](https://towardsai.net/p/data-science/how-when-and-why-should-you-normalize-standardize-rescale-your-data-3f083def38ff)  \n","**Normalization** is a good technique to use when you do not know the distribution of your data or when you know the distribution is not Gaussian (a bell curve). Normalization is useful when your data has varying scales and the algorithm you are using does not make assumptions about the distribution of your data, such as k-nearest neighbors and artificial neural networks.  \n","**Standardization** assumes that your data has a Gaussian (bell curve) distribution. This does not strictly have to be true, but the technique is more effective if your attribute distribution is Gaussian. Standardization is useful when your data has varying scales and the algorithm you are using does make assumptions about your data having a Gaussian distribution, such as linear regression, logistic regression, and linear discriminant analysis.\n"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DataScaler:\n","    def __init__(self, df: pd.DataFrame):\n","        self.df = df\n","        self.numeric_cols = df.select_dtypes(include=\"number\")\n","        self.scaler_std = StandardScaler()\n","        self.scaler_norm = MinMaxScaler()\n","\n","    def standardization(self, exclude_cols: list = []):\n","        self.df[self.numeric_cols.columns] = self.scaler_std.fit_transform(self.numeric_cols)\n","        self.df[exclude_cols] = self.numeric_cols[exclude_cols]\n","\n","    def normalization(self, exclude_cols: list = []):\n","        self.df[self.numeric_cols.columns] = self.scaler_norm.fit_transform(self.numeric_cols)\n","        self.df[exclude_cols] = self.numeric_cols[exclude_cols]\n","        \n","scalar = DataScaler(df)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Now we check the distribution of features to decide between normalizing and standardizing."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scalar.df.hist(bins=20, figsize=(20,15))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Since the important features (i.e. the one with high correlation with price feature) are mostly normally distributed, we use standardization."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scalar.normalization(exclude_cols=[\"price\"])\n","scalar.df.describe()"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["scalar.df.hist(bins=20, figsize=(20,15))\n","plt.show()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q2-4. Categorical values and encoding."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["There are many ways to encode the categorical features. Some of them are as follows:\n","\n","* `Label Encoding`: Assign a number to each category.  \n","This method is useful when the categories have an order. This is substituting each possible value of a categorical feature with a corresponding number. While label encoding is very simple, it is not always ideal because the numbers do not mean anything and can cause issues if used for calculating distance.  \n","  > Category 1: 0  \n","  > Category 2: 1  \n","  > Category 3: 2\n","\n","* `One-Hot Encoding`: Create a new feature for each category.\n","This method is useful when the categories don't have an order. It is the most useful method for the algorithms that use the distance between the data points, such as `KNN`. In this method, an additional feature is added for each categorical value and is marked 0 or 1. While this encoding is more proper, it adds a lot of new binary features which use more memory and can slow the dataset down.\n","    > Category 1: 1, 0, 0  \n","    > Category 2: 0, 1, 0  \n","    > Category 3: 0, 0, 1\n","\n","\n","* `Binary Encoding`: Encode the categories using binary numbers.\n","This method is useful when the categories don't have an order. It is somehow similar to the `One-Hot Encoding` method but it uses less memory.\n","    > Category 1: 00  \n","    > Category 2: 01  \n","    > Category 3: 10\n","\n","* `Frequency Encoding`: Encode the categories using the frequency of the categories.\n","This method is useful when the categories don't have an order. \n","    > Category 1: 0.5  \n","    > Category 2: 0.25  \n","    > Category 3: 0.25\n","\n","* `Target Encoding`: This is the process of replacing a categorical value with the mean of a target variable.  \n","  To do this, the data is grouped by each categorical value, and the average of a chosen target variable is calculated for that group. If the target is numerical, the categorical values are replaced with their corresponding average of the target. \n","   If the target is categorical, the values are replaced with their corresponding probability of the target. \n","This method is useful when the categories don't have an order. \n","    > Category 1: 0.5  \n","    > Category 2: 0.25  \n","    > Category 3: 0.75"]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class CategoricalEncoder:\n","    def __init__(self, df: pd.DataFrame):\n","        self.df = df\n","        self.cat_cols = df.select_dtypes(include=[\"category\", \"object\"])\n","        \n","        self.encoders = {\n","            \"label\": cat_enc.OrdinalEncoder(cols=self.cat_cols.columns),\n","            \"one-hot\": cat_enc.OneHotEncoder(cols=self.cat_cols.columns, use_cat_names=True),\n","            \"target\": cat_enc.TargetEncoder(cols=self.cat_cols.columns, min_samples_leaf=2, smoothing=1.1),\n","            \"frequency\": cat_enc.CountEncoder(cols=self.cat_cols.columns),\n","            \"binary\": cat_enc.BinaryEncoder(cols=self.cat_cols.columns),\n","        }\n","\n","    def encode(self, mode: str, target: str = None):\n","        if mode != \"target\":\n","            self.df[self.cat_cols.columns] = self.encoders[mode].fit_transform(self.cat_cols)\n","        else:\n","            self.df[self.cat_cols.columns] = self.encoders[mode].fit_transform(self.cat_cols, self.df[target])\n","\n","encoder = CategoricalEncoder(df)\n","encoder.encode(mode=\"label\")\n","display(df)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["###  Q2-5. Removing columns."]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["Some columns like id and Unnamed are unique and wont help us, others like longitude and latitude and zipcode are not crucial in determining price so we drop them. And also features that have low correlation with target."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df = df[price_corr.index.union([\"price\"])]\n","df.head(10)"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q2-6. Splitting the dataset into train and test sets.\n","There are some ways to split the dataset into train and test sets. Some of them are as follows:\n","\n","- Randomly split the dataset into train and test sets\n","    - This method is the most common method. But it has a problem. If we split the dataset randomly, the train and test sets may not have the same distribution.   \n","- Split the dataset based on the time\n","    - This method is useful when we have a time series dataset. But it is not useful in this case.  \n","- Split the dataset based on the target\n","    - This method is useful when we have an imbalanced dataset.\n","- Cross-validation \n","    - Which groups the data into *k* parts, and chooses one of them at each iteration and uses it as the test data, while using the rest as training data. *K-fold cross-validation* is simply splitting into *k* parts.\n","\n","Here we use the first method. And there are also several percentage for dividing, we use 80-20 here."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["class DataSplitter:\n","    def __init__(self, df: pd.DataFrame, train_percent: float = 0.8):\n","        self.data = df[df.columns.difference([\"price\"])]\n","        self.outcome_data = df[\"price\"]\n","        self.__split(train_percent)\n","\n","    def __split(self, train_percent: float):\n","        train_feat, test_feat, train_out, test_out = train_test_split(\n","                                                    self.data, self.outcome_data, train_size=train_percent, random_state=1)\n","        self.data_train = train_feat\n","        self.data_test = test_feat\n","        self.outcome_train = train_out\n","        self.outcome_test = test_out\n","        \n","dataSplitter = DataSplitter(df)\n"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["### Q2-7. Validation set.\n","In machine learning, a validation set is a subset of the data that is used to evaluate the performance of a trained model. The validation set is typically used to tune the hyperparameter of the model and to estimate the generalization error of the model.\n","The validation data is used to test the trained model before using the testing data.  \n","During this step, the classifier hyperparameters are adjusted.\n","The generalization error is the difference between the performance of the model on the training data and the performance of the model on new, unseen data. \n","To estimate the generalization error, we typically split the data into three sets: a training set, a validation set, and a test set. The training set is used to train the model, the validation set is used to tune the hyperparameters of the model and to estimate the generalization error, and the test set is used to evaluate the final performance of the model.\n","The validation set is used to evaluate the performance of the model during the training process. After each epoch of training, the model is evaluated on the validation set to see how well it is generalizing to new data. This allows us to monitor the performance of the model and to make adjustments to the hyperparameters as needed.\n","Once the model has been trained and the hyperparameters have been tuned using the validation set, we can evaluate the final performance of the model on the test set. The test set provides an unbiased estimate of the generalization error of the model, since it has not been used during the training or validation process."]},{"cell_type":"code","execution_count":null,"metadata":{},"outputs":[],"source":["df.describe()"]},{"attachments":{},"cell_type":"markdown","metadata":{},"source":["## Part 3. Training, Testing and Evaluating the Models."]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"b-TMX5-kgXe7"},"source":["### **Phase 1.** Linear Regression\n","Main form of simple linear regression function: \n","$$f(x) = \\alpha x + \\beta$$\n","\n","here we want to find the intercept($\\alpha$) and slope($\\beta$) by minimizing the derivation of the RSS function:\n","\n","- step 1: Compute RSS of the training data  \n","\n","$$ RSS = \\Sigma (y_i - (\\hat{\\beta} + \\hat{\\alpha} * x_i) )^2 $$\n","\n","Where $\\hat{\\alpha}$ is the estimated value of the constant term $\\alpha$ and $\\hat{\\beta}$ is the estimated value of the slope coefficient $\\beta$\n","\n","- step 2: Compute the derivatives of the RSS function in term of $\\underline{\\alpha}$ and $\\underline{\\beta}$, and set them equal to 0 to find the desired parameters\n","\n","$$ \\frac{\\partial RSS}{\\partial \\beta} = \\Sigma (-f(x_i) + \\hat{\\beta} + \\hat{\\alpha} * x_i) = 0$$\n","$$ \\to \\hat{\\beta} = \\hat{y} - \\hat{\\alpha} \\hat{x} \\to (1)$$\n","\n","\n","$$ \\frac{\\partial RSS}{\\partial \\alpha} = \\Sigma (-2 x_i y_i + 2 \\hat{\\beta} x_i + 2\\hat{\\alpha} x_i ^ 2) = 0 \\to (2)$$\n","\n","$$ (1) , (2) \\to \\hat{\\alpha} = \\frac{\\Sigma{(x_i - \\hat{x})(y_i - \\hat{y})}}{\\Sigma{(x_i - \\hat{x})^2}}\n","$$ \n","$$ \\hat{\\beta} = \\hat{y} - \\hat{\\alpha} \\hat{x}$$\n","\n"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"GdPAhX3KgXe9"},"source":["Based on the formula above, complete this function to compute the parameters of a simple linear regression"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"WiBbu2fagXfB"},"outputs":[],"source":["def simple_linear_regression(input_feature, output):\n","    # TO DO:\n","\n","    # compute the sum of input_feature and output\n","\n","    # compute the product of the output and the input_feature and its sum\n","\n","    # compute the squared value of the input_feature and its sum\n","\n","    # use the formula for the slope\n","\n","    # use the formula for the intercept\n","\n","    return (intercept, slope)"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"ihT-a1dvgXfE"},"source":["Now complete this function to predict the value of given data based on the calculated intercept and slope"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"YwHcast_gXfF"},"outputs":[],"source":["def get_regression_predictions(input_feature, intercept, slope):\n","    # TO DO:\n","\n","    # calculate the predicted values:\n","\n","    return predicted_values"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"SgIG_oJggXfF"},"source":["Now that we have a model and can make predictions let's evaluate our model using Root Mean Square Error (RSME). RMSE is the square root of the mean of the squared differences between the residuals and the residuals is just a fancy word for the difference between the predicted output and the true output.\n","\n","Complete the following function to compute the RSME of a simple linear regression model given the input_feature, output, intercept and slope:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"aFSnPatrgXfG"},"outputs":[],"source":["def get_root_mean_square_error(predicted_values, output):\n","    # TO DO:\n","\n","    # Compute the residuals (since we are squaring it doesn't matter which order you subtract)\n","\n","    # square the residuals and add them up\n","\n","    # find the mean of the above phrase\n","\n","    # calculate the root\n","\n","    return RMSE"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"VY_28bJFgXfH"},"source":["AS you might guessed, the RMSE has no bound and it is not easy to find out the percentage of fitting the model into data with it. instead, we use R2 score. The R2 score is calculated by comparing the sum of the squared differences between the actual and predicted values of the dependent variable to the total sum of squared differences between the actual and mean values of the dependent variable. Matematically, the R2 score formula is shown as follows:\n","\n","$$R^2 = 1 - \\frac{SSres}{SStot} = 1 - \\frac{\\sum_{i=1}^{n} (y_{i,true} - y_{i,pred})^2}{\\sum_{i=1}^{n} (y_{i,true} - \\bar{y}_{true})^2} $$"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"-Qpx03GxgXfH"},"source":["In this step, complete the following function to calculate the R2 score of a given input_feature, output, intercept, and slope:"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"_5hIou1AgXfJ"},"outputs":[],"source":["def get_r2_score(predicted_values, output):\n","    # TO DO:\n","\n","    # then compute the residuals (since we are squaring it doesn't matter which order you subtract)\n","\n","    # square the residuals and add them up -> SSres\n","\n","    # compute the SStot\n","\n","    # compute the R2 score value\n","\n","    return R2_score"]},{"attachments":{},"cell_type":"markdown","metadata":{"id":"1i90h7gugXfJ"},"source":["Now calculate the fitness of the model and explain the outputs"]},{"cell_type":"code","execution_count":null,"metadata":{"id":"QlpgW1c5gXfK","outputId":"565d94db-ce44-4287-c5ef-6e43107b1b3c"},"outputs":[],"source":["# TO DO:\n","\n","designated_feature_list = [\"sqft_living\", \"yr_built\", \"grade\", \"zipcode\"]\n","\n","for feature in designated_feature_list:\n","    # TO DO: calculate R2 score and RMSE for each given feature\n","    pass"]}],"metadata":{"colab":{"provenance":[]},"kernelspec":{"display_name":"Python 3","language":"python","name":"python3"},"language_info":{"codemirror_mode":{"name":"ipython","version":3},"file_extension":".py","mimetype":"text/x-python","name":"python","nbconvert_exporter":"python","pygments_lexer":"ipython3","version":"3.11.0"},"orig_nbformat":4},"nbformat":4,"nbformat_minor":0}
