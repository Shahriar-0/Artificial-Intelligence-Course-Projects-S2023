{
 "cells": [
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<html>\n",
    "<div>\n",
    "  <img src=\"https://www.engineersgarage.com/wp-content/uploads/2021/11/TCH36-01-scaled.jpg\" width=360px width=auto style=\"vertical-align: middle;\">\n",
    "  <span style=\"font-family: Georgia; font-size:30px; color: white;\"> <br/> University of Tehran <br/> AI_CA2 <br/> Spring 02 </span>\n",
    "</div>\n",
    "<span style=\"font-family: Georgia; font-size:15pt; color: white; vertical-align: middle;\"> low_mist - std id: 810100186 </span>\n",
    "</html>\n",
    "\n",
    "in this notebook we are to learn about adversarial search and minimax method to solve them."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Problem Description\n",
    "in this problem we have to play [othello](https://www.eothello.com) against computer which only does random moves. we have to find a way to defeat it and we will also look into some optimization like alpha-beta pruning in order to improve the running time of the algorithm. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import annotations\n",
    "import random\n",
    "import time\n",
    "import turtle\n",
    "import math\n",
    "import time\n",
    "from copy import deepcopy\n",
    "from enum import Enum"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Othello UI\n",
    "It will draw our board for us to have better experience using the app."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "class OthelloUI:\n",
    "    def __init__(self, board_size = 6, square_size = 60):\n",
    "        self.board_size = board_size\n",
    "        self.square_size = square_size\n",
    "        self.screen = turtle.Screen()\n",
    "        self.screen.setup(self.board_size * self.square_size + 50, self.board_size * self.square_size + 50)\n",
    "        self.screen.bgcolor('white')\n",
    "        self.screen.title('Othello low mist')\n",
    "        self.pen = turtle.Turtle()\n",
    "        self.pen.hideturtle()\n",
    "        self.pen.speed(0)\n",
    "        turtle.tracer(0, 0)\n",
    "\n",
    "    def draw_board(self, board):\n",
    "        self.pen.penup()\n",
    "        x, y = -self.board_size / 2 * self.square_size, self.board_size / 2 * self.square_size\n",
    "        for i in range(self.board_size):\n",
    "            self.pen.penup()\n",
    "            for j in range(self.board_size):\n",
    "                self.pen.goto(x + j * self.square_size, y - i * self.square_size)\n",
    "                self.pen.pendown()\n",
    "                self.pen.fillcolor('green')\n",
    "                self.pen.begin_fill()\n",
    "                self.pen.setheading(0)\n",
    "                for _ in range(4):\n",
    "                    self.pen.forward(self.square_size)\n",
    "                    self.pen.right(90)\n",
    "                self.pen.penup()\n",
    "                self.pen.end_fill()\n",
    "                self.pen.goto(x + j * self.square_size + self.square_size / 2,\n",
    "                              y - i * self.square_size - self.square_size + 5)\n",
    "                if board[i][j] == 1:\n",
    "                    self.pen.fillcolor('white')\n",
    "                    self.pen.begin_fill()\n",
    "                    self.pen.circle(self.square_size / 2 - 5)\n",
    "                    self.pen.end_fill()\n",
    "                elif board[i][j] == -1:\n",
    "                    self.pen.fillcolor('black')\n",
    "                    self.pen.begin_fill()\n",
    "                    self.pen.circle(self.square_size / 2 - 5)\n",
    "                    self.pen.end_fill()\n",
    "\n",
    "        turtle.update()"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## The Main Game Class\n",
    "The main class `Othello` is defined here which has different methods:\n",
    "- `constructor`: we initialize some useful values that will be used later.\n",
    "Some utilities for determining winner, getting valid moves, check for terminal and so forth.\n",
    "- `get_winner`\n",
    "- `get_valid_moves`\n",
    "- `terminal_test`\n",
    "- `get_cpu_move`\n",
    "- `get_human_move`  \n",
    "---\n",
    "And the main method is play which will run and gives us the result. 1 means that we have won, 0 indicates draw and -1 shows that we lost.  \n",
    "Since we want to run algorithm in different modes (i.e. with and without pruning) with different minimax depth and check their running times we put these two setter to alter some values.\n",
    "- `set_minimax_depth`\n",
    "- `set_pruning`  \n",
    "---\n",
    "And there are some functions for evaluation such as\n",
    "- `count_corners`\n",
    "- `count_borders`\n",
    "- `count_total`\n",
    "- `heuristic`: which uses all the above function to map any state to a float value  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "HUMAN, COMPUTER = 1, -1\n",
    "Move = tuple[int, int]\n",
    "\n",
    "class Othello:\n",
    "    def __init__(self, ui = False, minimax_depth = 1, prune = True):\n",
    "        self.size = 6\n",
    "        self.ui = OthelloUI(self.size) if ui else None\n",
    "        self.board = [[0 for _ in range(self.size)] for _ in range(self.size)]\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2) - 1] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2)] = 1\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2)] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2) - 1] = -1\n",
    "        self.current_turn = random.choice([1, -1])\n",
    "        self.minimax_depth = minimax_depth\n",
    "        self.prune = prune\n",
    "        self.CORNER_WEIGHT = 10\n",
    "        self.BORDER_WEIGHT = 2\n",
    "        self.TOTAL_WEIGHT = 1\n",
    "        self.WIN_HEURISTIC = 1000\n",
    "        self.seen_nodes = 0\n",
    "        \n",
    "    def set_minimax_depth(self, depth: int):\n",
    "        self.minimax_depth = depth\n",
    "        \n",
    "    def set_pruning(self, prune: bool):\n",
    "        self.prune = prune\n",
    "\n",
    "    def get_winner(self):\n",
    "        white_count = sum([row.count(HUMAN) for row in self.board])\n",
    "        black_count = sum([row.count(COMPUTER) for row in self.board])\n",
    "        if white_count > black_count:\n",
    "            return HUMAN\n",
    "        elif white_count < black_count:\n",
    "            return COMPUTER\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    def get_valid_moves(self, player):\n",
    "        moves = set()\n",
    "        for i in range(self.size):\n",
    "            for j in range(self.size):\n",
    "                if self.board[i][j] == 0:\n",
    "                    for di in [-1, 0, 1]:\n",
    "                        for dj in [-1, 0, 1]:\n",
    "                            if di == 0 and dj == 0:\n",
    "                                continue\n",
    "                            x, y = i, j\n",
    "                            captured = []\n",
    "                            while 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][\n",
    "                                    y + dj] == -player:\n",
    "                                captured.append((x + di, y + dj))\n",
    "                                x += di\n",
    "                                y += dj\n",
    "                            if 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][\n",
    "                                    y + dj] == player and len(captured) > 0:\n",
    "                                moves.add((i, j))\n",
    "        return list(moves)\n",
    "\n",
    "    def make_move(self, player, move):\n",
    "        i, j = move\n",
    "        self.board[i][j] = player\n",
    "        for di in [-1, 0, 1]:\n",
    "            for dj in [-1, 0, 1]:\n",
    "                if di == 0 and dj == 0:\n",
    "                    continue\n",
    "                x, y = i, j\n",
    "                captured = []\n",
    "                while 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][y + dj] == -player:\n",
    "                    captured.append((x + di, y + dj))\n",
    "                    x += di\n",
    "                    y += dj\n",
    "                if 0 <= x + di < self.size and 0 <= y + dj < self.size and self.board[x + di][y + dj] == player:\n",
    "                    for (cx, cy) in captured:\n",
    "                        self.board[cx][cy] = player\n",
    "\n",
    "    def get_cpu_move(self):\n",
    "        moves = self.get_valid_moves(COMPUTER)\n",
    "        if len(moves) == 0:\n",
    "            return None\n",
    "        return random.choice(moves)\n",
    "\n",
    "    def get_human_move(self):\n",
    "        value, move = self.minimax(self.minimax_depth, HUMAN)\n",
    "        return move\n",
    "    \n",
    "    def minimax(self, depth: int, turn: int, alpha: float = -math.inf, beta: float = math.inf) -> tuple[int, Move]:\n",
    "        self.seen_nodes += 1\n",
    "        if self.terminal_test():\n",
    "            value = self.WIN_HEURISTIC if self.get_winner() == HUMAN else -self.WIN_HEURISTIC\n",
    "            return value, None\n",
    "        \n",
    "        if depth <= 0:\n",
    "            return self.heuristic(), None\n",
    "        \n",
    "        backup_board = [[x for x in row] for row in self.board]\n",
    "        optimal_move = None\n",
    "        \n",
    "        if turn == HUMAN and len(self.get_valid_moves(turn)) == 0:\n",
    "            turn *= -1\n",
    "        \n",
    "        if turn == HUMAN:\n",
    "            node_value = -math.inf\n",
    "            for move in self.get_valid_moves(HUMAN):\n",
    "                self.make_move(HUMAN, move)\n",
    "                value, successor_move = self.minimax(depth - 1, COMPUTER, alpha, beta)\n",
    "                self.board = [[x for x in row] for row in backup_board]\n",
    "                if value > node_value:\n",
    "                    optimal_move = move\n",
    "                    node_value = value\n",
    "                    if self.prune and node_value >= beta:\n",
    "                        break\n",
    "                    alpha = max(alpha, value)\n",
    "                \n",
    "            return node_value, optimal_move \n",
    "        \n",
    "        elif turn == COMPUTER:\n",
    "            node_value = math.inf\n",
    "            for move in self.get_valid_moves(COMPUTER):\n",
    "                self.make_move(COMPUTER, move)\n",
    "                value, successor_move = self.minimax(depth - 1, HUMAN, alpha, beta)\n",
    "                self.board = [[x for x in row] for row in backup_board]\n",
    "                if value < node_value:\n",
    "                    optimal_move = move\n",
    "                    node_value = value\n",
    "                    if self.prune and node_value <= alpha:\n",
    "                        break\n",
    "                    beta = min(beta, value)\n",
    "                    \n",
    "            return node_value, optimal_move \n",
    "        \n",
    "    def heuristic(self) -> int:\n",
    "        human_corners = self.count_corners(HUMAN)\n",
    "        computer_corners = self.count_corners(COMPUTER)\n",
    "        corners_coefficient = (human_corners - computer_corners) \n",
    "        \n",
    "        human_total = self.count_total(HUMAN)\n",
    "        computer_total = self.count_total(COMPUTER)\n",
    "        total_coefficient = (human_total - computer_total)\n",
    "        \n",
    "        return self.CORNER_WEIGHT * corners_coefficient + self.TOTAL_WEIGHT * total_coefficient\n",
    "            #    self.BORDER_WEIGHT * self.count_empty() * (self.count_borders(HUMAN) - self.count_borders(COMPUTER)) + \\\n",
    "    \n",
    "    def count_corners(self, player: int) -> int:\n",
    "        sum = 0\n",
    "        sum += self.board[0][0] == player\n",
    "        sum += self.board[0][-1] == player\n",
    "        sum += self.board[-1][0] == player\n",
    "        sum += self.board[-1][-1] == player\n",
    "        return sum\n",
    "               \n",
    "    def count_borders(self, player: int) -> int:\n",
    "        sum = 0\n",
    "        for i in range(self.size):\n",
    "            sum += self.board[0][i] == player\n",
    "            sum += self.board[-1][i] == player\n",
    "            sum += self.board[i][0] == player\n",
    "            sum += self.board[i][-1] == player\n",
    "        return sum\n",
    "    \n",
    "    def count_total(self, player: int) -> int:\n",
    "        return sum(row.count(player) for row in self.board)\n",
    "        \n",
    "    def terminal_test(self):\n",
    "        return len(self.get_valid_moves(HUMAN)) == 0 and len(self.get_valid_moves(COMPUTER)) == 0\n",
    "\n",
    "    def play(self):\n",
    "        winner = None\n",
    "        while not self.terminal_test():\n",
    "            if self.ui:\n",
    "                self.ui.draw_board(self.board)\n",
    "            if self.current_turn == HUMAN:\n",
    "                move = self.get_human_move()\n",
    "                if move:\n",
    "                    self.make_move(self.current_turn, move)\n",
    "            else:\n",
    "                move = self.get_cpu_move()\n",
    "                if move:\n",
    "                    self.make_move(self.current_turn, move)\n",
    "            self.current_turn = -self.current_turn\n",
    "            if self.ui:\n",
    "                self.ui.draw_board(self.board)\n",
    "                time.sleep(1)\n",
    "                \n",
    "        winner = self.get_winner()\n",
    "        return winner\n",
    "    \n",
    "    def reset(self):\n",
    "        self.board = [[0 for _ in range(self.size)] for _ in range(self.size)]\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2) - 1] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2)] = 1\n",
    "        self.board[int(self.size / 2) - 1][int(self.size / 2)] = self.board[int(self.size / 2)][\n",
    "            int(self.size / 2) - 1] = -1\n",
    "        self.current_turn = random.choice([1, -1])\n",
    "        self.seen_nodes = 0"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In above class `minimax` is a recursive function that runs the algorithm.  \n",
    "It takes 4 parameters:\n",
    "\n",
    "- `depth`: The current depth (which decreases every time we go deeper into the tree)\n",
    "- `turn`: Specifies which player's turn it is.\n",
    "- `alpha`: The maximum of the ancestor's branches along the path to the current node so far.\n",
    "- `beta`: The minimum of the ancestor's branches along the path to the current node so far.\n",
    "\n",
    "If the `prune` flag is false, alpha and beta do not do anything.\n",
    "\n",
    "At each recursion, it is checked whether it's over or not.\n",
    "If we have reached the depth limit of the tree and the game in that path is not over yet, we use the `heuristic` function to score the current state.  \n",
    "Next if we it's someone's turn but we don't have any available move we change the turn (since terminate is not called we are sure that are player still has a move). Note that it's different from the one in the play method.  \n",
    "Next, based on the current turn, for each move we change and minimax is recursively called again and then we undo the changes that me made. alpha and beta will be updated accordingly. they are first initialized to $\\pm\\infty$ so we can update them and also depth will get othello.minimax_depth when it's called.   \n",
    "`minimax` returns a tuple of the score and move.  \n",
    "We use minimax algorithm for the games that the opponent acts smartly. However, in this game, the opponent acts randomly so better choice would have been expectimax because we are somehow overprotective in this algorithm."
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Results "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "TOTAL_TESTS = 100\n",
    "\n",
    "def test(self, depth: int, prune: bool = True, num_of_test: int = TOTAL_TESTS) -> tuple[float, float, int]:\n",
    "        ui = self.ui\n",
    "        self.ui = None\n",
    "        \n",
    "        win = 0\n",
    "        time_elapsed = 0\n",
    "        seen_nodes = 0\n",
    "        self.set_minimax_depth(depth)\n",
    "        self.set_pruning(prune)\n",
    "        \n",
    "        for _ in range(num_of_test):\n",
    "            start = time.time()\n",
    "            win += (self.play() == HUMAN)\n",
    "            time_elapsed += time.time() - start\n",
    "            seen_nodes += self.seen_nodes\n",
    "            self.reset()\n",
    "            \n",
    "        self.ui = ui\n",
    "        \n",
    "        return time_elapsed / num_of_test, win / num_of_test, seen_nodes // num_of_test       \n",
    "    \n",
    "Othello.test = test"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### without pruning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for depth 1:\n",
      "average time for each game was: 0.006669454574584961\n",
      "win percentage: 0.96\n",
      "average number of seen nodes: 96\n",
      "------------------------------------------\n",
      "for depth 3:\n",
      "average time for each game was: 0.17674270153045654\n",
      "win percentage: 0.96\n",
      "average number of seen nodes: 3607\n",
      "------------------------------------------\n"
     ]
    },
    {
     "ename": "KeyboardInterrupt",
     "evalue": "",
     "output_type": "error",
     "traceback": [
      "\u001b[1;31m---------------------------------------------------------------------------\u001b[0m",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m                         Traceback (most recent call last)",
      "Cell \u001b[1;32mIn[5], line 5\u001b[0m\n\u001b[0;32m      3\u001b[0m othello \u001b[39m=\u001b[39m Othello()\n\u001b[0;32m      4\u001b[0m \u001b[39mfor\u001b[39;00m i \u001b[39min\u001b[39;00m without_prune_depth:\n\u001b[1;32m----> 5\u001b[0m     average_time, win_percentage, seen_nodes \u001b[39m=\u001b[39m othello\u001b[39m.\u001b[39;49mtest(i, prune \u001b[39m=\u001b[39;49m \u001b[39mFalse\u001b[39;49;00m, num_of_test \u001b[39m=\u001b[39;49m TOTAL_TESTS \u001b[39m/\u001b[39;49m\u001b[39m/\u001b[39;49m \u001b[39m10\u001b[39;49m \u001b[39mif\u001b[39;49;00m i \u001b[39m==\u001b[39;49m \u001b[39m5\u001b[39;49m \u001b[39melse\u001b[39;49;00m TOTAL_TESTS)\n\u001b[0;32m      6\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39mfor depth \u001b[39m\u001b[39m{\u001b[39;00mi\u001b[39m}\u001b[39;00m\u001b[39m:\u001b[39m\u001b[39m\"\u001b[39m)\n\u001b[0;32m      7\u001b[0m     \u001b[39mprint\u001b[39m(\u001b[39mf\u001b[39m\u001b[39m\"\u001b[39m\u001b[39maverage time for each game was: \u001b[39m\u001b[39m{\u001b[39;00maverage_time\u001b[39m}\u001b[39;00m\u001b[39m\"\u001b[39m)\n",
      "Cell \u001b[1;32mIn[4], line 15\u001b[0m, in \u001b[0;36mtest\u001b[1;34m(self, depth, prune, num_of_test)\u001b[0m\n\u001b[0;32m     13\u001b[0m \u001b[39mfor\u001b[39;00m _ \u001b[39min\u001b[39;00m \u001b[39mrange\u001b[39m(num_of_test):\n\u001b[0;32m     14\u001b[0m     start \u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime()\n\u001b[1;32m---> 15\u001b[0m     win \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m (\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mplay() \u001b[39m==\u001b[39m HUMAN)\n\u001b[0;32m     16\u001b[0m     time_elapsed \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m time\u001b[39m.\u001b[39mtime() \u001b[39m-\u001b[39m start\n\u001b[0;32m     17\u001b[0m     seen_nodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen_nodes\n",
      "Cell \u001b[1;32mIn[3], line 172\u001b[0m, in \u001b[0;36mOthello.play\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    170\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mui\u001b[39m.\u001b[39mdraw_board(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard)\n\u001b[0;32m    171\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_turn \u001b[39m==\u001b[39m HUMAN:\n\u001b[1;32m--> 172\u001b[0m     move \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_human_move()\n\u001b[0;32m    173\u001b[0m     \u001b[39mif\u001b[39;00m move:\n\u001b[0;32m    174\u001b[0m         \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_move(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mcurrent_turn, move)\n",
      "Cell \u001b[1;32mIn[3], line 83\u001b[0m, in \u001b[0;36mOthello.get_human_move\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m     82\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mget_human_move\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m---> 83\u001b[0m     value, move \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax_depth, HUMAN)\n\u001b[0;32m     84\u001b[0m     \u001b[39mreturn\u001b[39;00m move\n",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m, in \u001b[0;36mOthello.minimax\u001b[1;34m(self, depth, turn, alpha, beta)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_valid_moves(HUMAN):\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_move(HUMAN, move)\n\u001b[1;32m--> 105\u001b[0m     value, successor_move \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(depth \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, COMPUTER, alpha, beta)\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard \u001b[39m=\u001b[39m [[x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m row] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m backup_board]\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39m>\u001b[39m node_value:\n",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m, in \u001b[0;36mOthello.minimax\u001b[1;34m(self, depth, turn, alpha, beta)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_valid_moves(COMPUTER):\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_move(COMPUTER, move)\n\u001b[1;32m--> 120\u001b[0m     value, successor_move \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(depth \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, HUMAN, alpha, beta)\n\u001b[0;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard \u001b[39m=\u001b[39m [[x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m row] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m backup_board]\n\u001b[0;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39m<\u001b[39m node_value:\n",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m, in \u001b[0;36mOthello.minimax\u001b[1;34m(self, depth, turn, alpha, beta)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_valid_moves(HUMAN):\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_move(HUMAN, move)\n\u001b[1;32m--> 105\u001b[0m     value, successor_move \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(depth \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, COMPUTER, alpha, beta)\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard \u001b[39m=\u001b[39m [[x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m row] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m backup_board]\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39m>\u001b[39m node_value:\n",
      "Cell \u001b[1;32mIn[3], line 120\u001b[0m, in \u001b[0;36mOthello.minimax\u001b[1;34m(self, depth, turn, alpha, beta)\u001b[0m\n\u001b[0;32m    118\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_valid_moves(COMPUTER):\n\u001b[0;32m    119\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_move(COMPUTER, move)\n\u001b[1;32m--> 120\u001b[0m     value, successor_move \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(depth \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, HUMAN, alpha, beta)\n\u001b[0;32m    121\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard \u001b[39m=\u001b[39m [[x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m row] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m backup_board]\n\u001b[0;32m    122\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39m<\u001b[39m node_value:\n",
      "Cell \u001b[1;32mIn[3], line 105\u001b[0m, in \u001b[0;36mOthello.minimax\u001b[1;34m(self, depth, turn, alpha, beta)\u001b[0m\n\u001b[0;32m    103\u001b[0m \u001b[39mfor\u001b[39;00m move \u001b[39min\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_valid_moves(HUMAN):\n\u001b[0;32m    104\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mmake_move(HUMAN, move)\n\u001b[1;32m--> 105\u001b[0m     value, successor_move \u001b[39m=\u001b[39m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mminimax(depth \u001b[39m-\u001b[39;49m \u001b[39m1\u001b[39;49m, COMPUTER, alpha, beta)\n\u001b[0;32m    106\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard \u001b[39m=\u001b[39m [[x \u001b[39mfor\u001b[39;00m x \u001b[39min\u001b[39;00m row] \u001b[39mfor\u001b[39;00m row \u001b[39min\u001b[39;00m backup_board]\n\u001b[0;32m    107\u001b[0m     \u001b[39mif\u001b[39;00m value \u001b[39m>\u001b[39m node_value:\n",
      "Cell \u001b[1;32mIn[3], line 88\u001b[0m, in \u001b[0;36mOthello.minimax\u001b[1;34m(self, depth, turn, alpha, beta)\u001b[0m\n\u001b[0;32m     86\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mminimax\u001b[39m(\u001b[39mself\u001b[39m, depth: \u001b[39mint\u001b[39m, turn: \u001b[39mint\u001b[39m, alpha: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m \u001b[39m-\u001b[39mmath\u001b[39m.\u001b[39minf, beta: \u001b[39mfloat\u001b[39m \u001b[39m=\u001b[39m math\u001b[39m.\u001b[39minf) \u001b[39m-\u001b[39m\u001b[39m>\u001b[39m \u001b[39mtuple\u001b[39m[\u001b[39mint\u001b[39m, Move]:\n\u001b[0;32m     87\u001b[0m     \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mseen_nodes \u001b[39m+\u001b[39m\u001b[39m=\u001b[39m \u001b[39m1\u001b[39m\n\u001b[1;32m---> 88\u001b[0m     \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mterminal_test():\n\u001b[0;32m     89\u001b[0m         value \u001b[39m=\u001b[39m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWIN_HEURISTIC \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_winner() \u001b[39m==\u001b[39m HUMAN \u001b[39melse\u001b[39;00m \u001b[39m-\u001b[39m\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mWIN_HEURISTIC\n\u001b[0;32m     90\u001b[0m         \u001b[39mreturn\u001b[39;00m value, \u001b[39mNone\u001b[39;00m\n",
      "Cell \u001b[1;32mIn[3], line 164\u001b[0m, in \u001b[0;36mOthello.terminal_test\u001b[1;34m(self)\u001b[0m\n\u001b[0;32m    163\u001b[0m \u001b[39mdef\u001b[39;00m \u001b[39mterminal_test\u001b[39m(\u001b[39mself\u001b[39m):\n\u001b[1;32m--> 164\u001b[0m     \u001b[39mreturn\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39;49m\u001b[39m.\u001b[39;49mget_valid_moves(HUMAN)) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m \u001b[39mlen\u001b[39m(\u001b[39mself\u001b[39m\u001b[39m.\u001b[39mget_valid_moves(COMPUTER)) \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m\n",
      "Cell \u001b[1;32mIn[3], line 44\u001b[0m, in \u001b[0;36mOthello.get_valid_moves\u001b[1;34m(self, player)\u001b[0m\n\u001b[0;32m     42\u001b[0m \u001b[39mif\u001b[39;00m \u001b[39mself\u001b[39m\u001b[39m.\u001b[39mboard[i][j] \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     43\u001b[0m     \u001b[39mfor\u001b[39;00m di \u001b[39min\u001b[39;00m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]:\n\u001b[1;32m---> 44\u001b[0m         \u001b[39mfor\u001b[39;00m dj \u001b[39min\u001b[39;00m [\u001b[39m-\u001b[39m\u001b[39m1\u001b[39m, \u001b[39m0\u001b[39m, \u001b[39m1\u001b[39m]:\n\u001b[0;32m     45\u001b[0m             \u001b[39mif\u001b[39;00m di \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m \u001b[39mand\u001b[39;00m dj \u001b[39m==\u001b[39m \u001b[39m0\u001b[39m:\n\u001b[0;32m     46\u001b[0m                 \u001b[39mcontinue\u001b[39;00m\n",
      "\u001b[1;31mKeyboardInterrupt\u001b[0m: "
     ]
    }
   ],
   "source": [
    "without_prune_depth = [1, 3, 5]\n",
    "\n",
    "othello = Othello()\n",
    "for i in without_prune_depth:\n",
    "    average_time, win_percentage, seen_nodes = othello.test(i, prune = False, num_of_test = TOTAL_TESTS // 10 if i == 5 else TOTAL_TESTS)\n",
    "    print(f\"for depth {i}:\")\n",
    "    print(f\"average time for each game was: {average_time}\")\n",
    "    print(f\"win percentage: {win_percentage}\")\n",
    "    print(f\"average number of seen nodes: {seen_nodes}\")\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### with pruning results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "for depth 1:\n",
      "average time for each game was: 0.011689422130584716\n",
      "win percentage: 0.95\n",
      "average number of seen nodes: 97\n",
      "------------------------------------------\n",
      "for depth 3:\n",
      "average time for each game was: 0.21613157272338868\n",
      "win percentage: 1.0\n",
      "average number of seen nodes: 1562\n",
      "------------------------------------------\n",
      "for depth 5:\n",
      "average time for each game was: 2.18675642490387\n",
      "win percentage: 1.0\n",
      "average number of seen nodes: 18509\n",
      "------------------------------------------\n",
      "for depth 7:\n",
      "average time for each game was: 19.176905393600464\n",
      "win percentage: 1.0\n",
      "average number of seen nodes: 235853\n",
      "------------------------------------------\n"
     ]
    }
   ],
   "source": [
    "with_prune_depth = [1, 3, 5, 7]\n",
    "\n",
    "for i in with_prune_depth:\n",
    "    average_time, win_percentage, seen_nodes  = othello.test(i, prune = True, num_of_test = TOTAL_TESTS // 2 if i == 7 else TOTAL_TESTS)\n",
    "    print(f\"for depth {i}:\")\n",
    "    print(f\"average time for each game was: {average_time}\")\n",
    "    print(f\"win percentage: {win_percentage}\")\n",
    "    print(f\"average number of seen nodes: {seen_nodes}\")\n",
    "    print(\"------------------------------------------\")"
   ]
  },
  {
   "attachments": {},
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Questions\n",
    "\n",
    "### 1. What did you take into consideration while writing heuristic function?\n",
    "  number of conquered corners with high coefficient since it can never be owned by our rival and it can be influential in the end. And obviously total number of conquered cells.\n",
    "\n",
    "### 2. What is the depth effect on total seen nodes, time, winning percentage? \n",
    "  As expected as we dive deeper and check more steps ahead we will have a better chance of winning, but everything comes with a price, the more nodes will get checked which means more memory and more time for checking them. We have to restore balance between them based on our needs. you can check the numbers and see the difference.\n",
    "\n",
    "### 3. What is the effect of children orders in performance as we use alpha beta pruning?\n",
    "   Worst ordering: In some cases, alpha-beta pruning algorithm does not prune any of the leaves of the tree, and works exactly as minimax algorithm. In this case, it also consumes more time because of alpha-beta factors, such a move of pruning is called worst ordering. In this case, the best move occurs on the right side of the tree. The time complexity for such an order is O(bm).\n",
    "   Ideal ordering: The ideal ordering for alpha-beta pruning occurs when lots of pruning happens in the tree, and best moves occur at the left side of the tree. We apply DFS hence it first search left of the tree and go deep twice as minimax algorithm in the same amount of time. Complexity in ideal ordering is O(bm/2). you can get more information from this [link](https://www.javatpoint.com/ai-alpha-beta-pruning).\n",
    "\n",
    "### 4. How does the branching factor change during the game?\n",
    "   as we get to the middle of the game it gets bigger since there are more options compare to the beginning. But as we get closer to the end it reduces cause there are not too many empty cells, So it has kinda a form of normal distribution.\n",
    "\n",
    "### 5. How does alpha-beta pruning helps to reduce time without dropping accuracy? \n",
    "   in alpha-beta pruning we only prune branches that we are sure we will never need for instance if the father of a node is max node so we are min node, and we seen a value which is less than that of our father, so regardless of what we see next we will never be able to change our father's value so it's useless to check them.\n",
    "\n",
    "### 6. Was this algorithm efficient for this project?\n",
    "   since our rival is acting randomly so the best choice as explained before is expectimax because we are too protective and a little risk can be beneficial."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  },
  "orig_nbformat": 4
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
